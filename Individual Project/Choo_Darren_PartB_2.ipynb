{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# CS4001/4042 Assignment 1, Part B, Q2\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EycCozG06Duu",
    "outputId": "7ce1eb32-0b4b-4d55-f5cd-6b3d3c6b7b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-widedeep\n",
      "  Downloading pytorch_widedeep-1.3.2-py3-none-any.whl (21.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.2.2)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.3.2)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (3.6.1)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.8.0.76)\n",
      "Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (0.5.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.66.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (0.15.2+cu118)\n",
      "Collecting einops (from pytorch-widedeep)\n",
      "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.15.0)\n",
      "Collecting torchmetrics (from pytorch-widedeep)\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (9.0.0)\n",
      "Collecting fastparquet>=0.8.1 (from pytorch-widedeep)\n",
      "  Downloading fastparquet-2023.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cramjam>=2.3 (from fastparquet>=0.8.1->pytorch-widedeep)\n",
      "  Downloading cramjam-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2023.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet>=0.8.1->pytorch-widedeep) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-widedeep) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3.post1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pytorch-widedeep) (6.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (0.10.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (67.7.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (3.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch-widedeep) (3.27.4.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch-widedeep) (16.0.6)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics->pytorch-widedeep)\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->pytorch-widedeep) (9.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep) (0.1.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy->pytorch-widedeep) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->pytorch-widedeep) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch-widedeep) (1.3.0)\n",
      "Installing collected packages: lightning-utilities, einops, cramjam, fastparquet, torchmetrics, pytorch-widedeep\n",
      "Successfully installed cramjam-2.7.0 einops-0.7.0 fastparquet-2023.8.0 lightning-utilities-0.9.0 pytorch-widedeep-1.3.2 torchmetrics-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lq0elU0J53Yo",
    "outputId": "f62c6fb0-ccdb-496b-b25b-fc99769b7abd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_oYG6lNIh7Mp",
    "outputId": "e36b215c-6ac6-4f2f-e5a3-f89643b2179f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020    23313\n",
      "2019    22168\n",
      "2018    21552\n",
      "2017    20337\n",
      "Name: year, dtype: int64 \n",
      "\n",
      "2021    29057\n",
      "2022    26702\n",
      "2023    16424\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# TODO: Enter your code here\n",
    "\n",
    "train_df = df[df[\"year\"] <= 2020]\n",
    "test_df = df[df[\"year\"] >= 2021]\n",
    "\n",
    "print(train_df[\"year\"].value_counts(), \"\\n\")\n",
    "print(test_df[\"year\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBY1iqUXtYWn",
    "outputId": "7b2b67ef-120c-485c-a78b-f77680227ec4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:334: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 1366/1366 [00:29<00:00, 45.81it/s, loss=2.25e+5]\n",
      "epoch 2: 100%|██████████| 1366/1366 [00:19<00:00, 70.46it/s, loss=9.4e+4]\n",
      "epoch 3: 100%|██████████| 1366/1366 [00:16<00:00, 81.74it/s, loss=8.34e+4]\n",
      "epoch 4: 100%|██████████| 1366/1366 [00:16<00:00, 82.59it/s, loss=7.81e+4]\n",
      "epoch 5: 100%|██████████| 1366/1366 [00:16<00:00, 82.35it/s, loss=7.53e+4]\n",
      "epoch 6: 100%|██████████| 1366/1366 [00:16<00:00, 82.28it/s, loss=7.24e+4]\n",
      "epoch 7: 100%|██████████| 1366/1366 [00:17<00:00, 76.09it/s, loss=7.09e+4]\n",
      "epoch 8: 100%|██████████| 1366/1366 [00:18<00:00, 75.42it/s, loss=6.95e+4]\n",
      "epoch 9: 100%|██████████| 1366/1366 [00:16<00:00, 82.33it/s, loss=6.85e+4]\n",
      "epoch 10: 100%|██████████| 1366/1366 [00:16<00:00, 82.24it/s, loss=6.84e+4]\n",
      "epoch 11: 100%|██████████| 1366/1366 [00:16<00:00, 81.28it/s, loss=6.76e+4]\n",
      "epoch 12: 100%|██████████| 1366/1366 [00:16<00:00, 82.40it/s, loss=6.7e+4]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:16<00:00, 81.71it/s, loss=6.63e+4]\n",
      "epoch 14: 100%|██████████| 1366/1366 [00:16<00:00, 82.17it/s, loss=6.6e+4]\n",
      "epoch 15: 100%|██████████| 1366/1366 [00:16<00:00, 81.35it/s, loss=6.55e+4]\n",
      "epoch 16: 100%|██████████| 1366/1366 [00:16<00:00, 82.95it/s, loss=6.52e+4]\n",
      "epoch 17: 100%|██████████| 1366/1366 [00:16<00:00, 81.76it/s, loss=6.52e+4]\n",
      "epoch 18: 100%|██████████| 1366/1366 [00:17<00:00, 78.69it/s, loss=6.5e+4]\n",
      "epoch 19: 100%|██████████| 1366/1366 [00:17<00:00, 80.11it/s, loss=6.48e+4]\n",
      "epoch 20: 100%|██████████| 1366/1366 [00:16<00:00, 80.93it/s, loss=6.44e+4]\n",
      "epoch 21: 100%|██████████| 1366/1366 [00:16<00:00, 80.91it/s, loss=6.44e+4]\n",
      "epoch 22: 100%|██████████| 1366/1366 [00:17<00:00, 78.29it/s, loss=6.4e+4]\n",
      "epoch 23: 100%|██████████| 1366/1366 [00:16<00:00, 81.84it/s, loss=6.39e+4]\n",
      "epoch 24: 100%|██████████| 1366/1366 [00:16<00:00, 80.83it/s, loss=6.36e+4]\n",
      "epoch 25: 100%|██████████| 1366/1366 [00:16<00:00, 80.92it/s, loss=6.36e+4]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:16<00:00, 82.33it/s, loss=6.33e+4]\n",
      "epoch 27: 100%|██████████| 1366/1366 [00:16<00:00, 82.20it/s, loss=6.34e+4]\n",
      "epoch 28: 100%|██████████| 1366/1366 [00:17<00:00, 79.69it/s, loss=6.3e+4]\n",
      "epoch 29: 100%|██████████| 1366/1366 [00:18<00:00, 75.32it/s, loss=6.29e+4]\n",
      "epoch 30: 100%|██████████| 1366/1366 [00:16<00:00, 81.28it/s, loss=6.27e+4]\n",
      "epoch 31: 100%|██████████| 1366/1366 [00:17<00:00, 80.07it/s, loss=6.27e+4]\n",
      "epoch 32: 100%|██████████| 1366/1366 [00:17<00:00, 79.24it/s, loss=6.25e+4]\n",
      "epoch 33: 100%|██████████| 1366/1366 [00:16<00:00, 80.70it/s, loss=6.21e+4]\n",
      "epoch 34: 100%|██████████| 1366/1366 [00:17<00:00, 80.29it/s, loss=6.22e+4]\n",
      "epoch 35: 100%|██████████| 1366/1366 [00:16<00:00, 81.30it/s, loss=6.21e+4]\n",
      "epoch 36: 100%|██████████| 1366/1366 [00:17<00:00, 79.77it/s, loss=6.17e+4]\n",
      "epoch 37: 100%|██████████| 1366/1366 [00:16<00:00, 81.05it/s, loss=6.17e+4]\n",
      "epoch 38: 100%|██████████| 1366/1366 [00:17<00:00, 80.14it/s, loss=6.16e+4]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:17<00:00, 79.71it/s, loss=6.16e+4]\n",
      "epoch 40: 100%|██████████| 1366/1366 [00:18<00:00, 74.05it/s, loss=6.15e+4]\n",
      "epoch 41: 100%|██████████| 1366/1366 [00:17<00:00, 80.14it/s, loss=6.14e+4]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:16<00:00, 80.37it/s, loss=6.11e+4]\n",
      "epoch 43: 100%|██████████| 1366/1366 [00:17<00:00, 80.31it/s, loss=6.08e+4]\n",
      "epoch 44: 100%|██████████| 1366/1366 [00:17<00:00, 80.02it/s, loss=6.09e+4]\n",
      "epoch 45: 100%|██████████| 1366/1366 [00:17<00:00, 79.07it/s, loss=6.08e+4]\n",
      "epoch 46: 100%|██████████| 1366/1366 [00:17<00:00, 79.88it/s, loss=6.06e+4]\n",
      "epoch 47: 100%|██████████| 1366/1366 [00:17<00:00, 80.32it/s, loss=6.07e+4]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:17<00:00, 79.98it/s, loss=6.04e+4]\n",
      "epoch 49: 100%|██████████| 1366/1366 [00:17<00:00, 79.71it/s, loss=6.02e+4]\n",
      "epoch 50: 100%|██████████| 1366/1366 [00:17<00:00, 77.49it/s, loss=6.01e+4]\n",
      "epoch 51: 100%|██████████| 1366/1366 [00:18<00:00, 74.99it/s, loss=5.98e+4]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:17<00:00, 77.10it/s, loss=5.99e+4]\n",
      "epoch 53: 100%|██████████| 1366/1366 [00:17<00:00, 76.85it/s, loss=5.97e+4]\n",
      "epoch 54: 100%|██████████| 1366/1366 [00:17<00:00, 76.82it/s, loss=5.94e+4]\n",
      "epoch 55: 100%|██████████| 1366/1366 [00:18<00:00, 75.41it/s, loss=5.93e+4]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:17<00:00, 76.05it/s, loss=5.94e+4]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:18<00:00, 75.72it/s, loss=5.92e+4]\n",
      "epoch 58: 100%|██████████| 1366/1366 [00:18<00:00, 75.39it/s, loss=5.91e+4]\n",
      "epoch 59: 100%|██████████| 1366/1366 [00:18<00:00, 74.53it/s, loss=5.88e+4]\n",
      "epoch 60: 100%|██████████| 1366/1366 [00:18<00:00, 75.36it/s, loss=5.89e+4]\n",
      "epoch 61: 100%|██████████| 1366/1366 [00:19<00:00, 71.41it/s, loss=5.88e+4]\n",
      "epoch 62: 100%|██████████| 1366/1366 [00:18<00:00, 74.93it/s, loss=5.87e+4]\n",
      "epoch 63: 100%|██████████| 1366/1366 [00:18<00:00, 75.32it/s, loss=5.85e+4]\n",
      "epoch 64: 100%|██████████| 1366/1366 [00:18<00:00, 75.60it/s, loss=5.84e+4]\n",
      "epoch 65: 100%|██████████| 1366/1366 [00:18<00:00, 75.24it/s, loss=5.84e+4]\n",
      "epoch 66: 100%|██████████| 1366/1366 [00:17<00:00, 76.44it/s, loss=5.82e+4]\n",
      "epoch 67: 100%|██████████| 1366/1366 [00:18<00:00, 75.51it/s, loss=5.79e+4]\n",
      "epoch 68: 100%|██████████| 1366/1366 [00:18<00:00, 75.26it/s, loss=5.77e+4]\n",
      "epoch 69: 100%|██████████| 1366/1366 [00:17<00:00, 76.06it/s, loss=5.73e+4]\n",
      "epoch 70: 100%|██████████| 1366/1366 [00:17<00:00, 76.01it/s, loss=5.67e+4]\n",
      "epoch 71: 100%|██████████| 1366/1366 [00:19<00:00, 71.55it/s, loss=5.61e+4]\n",
      "epoch 72: 100%|██████████| 1366/1366 [00:18<00:00, 75.56it/s, loss=5.53e+4]\n",
      "epoch 73: 100%|██████████| 1366/1366 [00:18<00:00, 74.52it/s, loss=5.46e+4]\n",
      "epoch 74: 100%|██████████| 1366/1366 [00:17<00:00, 76.25it/s, loss=5.38e+4]\n",
      "epoch 75: 100%|██████████| 1366/1366 [00:17<00:00, 76.46it/s, loss=5.32e+4]\n",
      "epoch 76: 100%|██████████| 1366/1366 [00:18<00:00, 74.98it/s, loss=5.28e+4]\n",
      "epoch 77: 100%|██████████| 1366/1366 [00:18<00:00, 75.70it/s, loss=5.24e+4]\n",
      "epoch 78: 100%|██████████| 1366/1366 [00:18<00:00, 75.72it/s, loss=5.23e+4]\n",
      "epoch 79: 100%|██████████| 1366/1366 [00:18<00:00, 74.87it/s, loss=5.2e+4]\n",
      "epoch 80: 100%|██████████| 1366/1366 [00:17<00:00, 76.14it/s, loss=5.2e+4]\n",
      "epoch 81: 100%|██████████| 1366/1366 [00:19<00:00, 71.17it/s, loss=5.16e+4]\n",
      "epoch 82: 100%|██████████| 1366/1366 [00:18<00:00, 73.92it/s, loss=5.17e+4]\n",
      "epoch 83: 100%|██████████| 1366/1366 [00:18<00:00, 75.34it/s, loss=5.17e+4]\n",
      "epoch 84: 100%|██████████| 1366/1366 [00:18<00:00, 75.55it/s, loss=5.15e+4]\n",
      "epoch 85: 100%|██████████| 1366/1366 [00:18<00:00, 74.06it/s, loss=5.12e+4]\n",
      "epoch 86: 100%|██████████| 1366/1366 [00:18<00:00, 75.00it/s, loss=5.13e+4]\n",
      "epoch 87: 100%|██████████| 1366/1366 [00:18<00:00, 73.99it/s, loss=5.12e+4]\n",
      "epoch 88: 100%|██████████| 1366/1366 [00:18<00:00, 74.80it/s, loss=5.12e+4]\n",
      "epoch 89: 100%|██████████| 1366/1366 [00:18<00:00, 75.00it/s, loss=5.08e+4]\n",
      "epoch 90: 100%|██████████| 1366/1366 [00:18<00:00, 75.00it/s, loss=5.09e+4]\n",
      "epoch 91: 100%|██████████| 1366/1366 [00:18<00:00, 74.10it/s, loss=5.08e+4]\n",
      "epoch 92: 100%|██████████| 1366/1366 [00:18<00:00, 72.00it/s, loss=5.07e+4]\n",
      "epoch 93: 100%|██████████| 1366/1366 [00:18<00:00, 74.11it/s, loss=5.06e+4]\n",
      "epoch 94: 100%|██████████| 1366/1366 [00:17<00:00, 76.10it/s, loss=5.07e+4]\n",
      "epoch 95: 100%|██████████| 1366/1366 [00:18<00:00, 74.26it/s, loss=5.05e+4]\n",
      "epoch 96: 100%|██████████| 1366/1366 [00:18<00:00, 75.01it/s, loss=5.01e+4]\n",
      "epoch 97: 100%|██████████| 1366/1366 [00:18<00:00, 74.96it/s, loss=5.03e+4]\n",
      "epoch 98: 100%|██████████| 1366/1366 [00:18<00:00, 73.67it/s, loss=5.03e+4]\n",
      "epoch 99: 100%|██████████| 1366/1366 [00:18<00:00, 74.83it/s, loss=5.03e+4]\n",
      "epoch 100: 100%|██████████| 1366/1366 [00:18<00:00, 74.53it/s, loss=5.01e+4]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "\n",
    "\n",
    "#creating TabPreprocessor\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols = [\"month\", \"town\", \"flat_model_type\", \"storey_range\"],\n",
    "    continuous_cols = [\"dist_to_nearest_stn\", \"dist_to_dhoby\", \"degree_centrality\", \"eigenvector_centrality\", \"remaining_lease_years\", \"floor_area_sqm\"]\n",
    ")\n",
    "\n",
    "#transformer the training dataset\n",
    "X_tab = tab_preprocessor.fit_transform(train_df)\n",
    "\n",
    "\n",
    "#creating our model\n",
    "tab_mlp = TabMlp(\n",
    "    mlp_hidden_dims=[200,100],\n",
    "    column_idx = tab_preprocessor.column_idx,\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=tab_preprocessor.continuous_cols,\n",
    "    )\n",
    "\n",
    "model = WideDeep(deeptabular=tab_mlp)\n",
    "\n",
    "\n",
    "#creating our trainer\n",
    "trainer = Trainer(model, objective=\"rmse\",  num_workers = 0)\n",
    "\n",
    "#fitting the trainer\n",
    "trainer.fit(\n",
    "    X_tab = X_tab,\n",
    "    n_epochs = 100,\n",
    "    batch_size = 64,\n",
    "    target = train_df[\"resale_price\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving / Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "o8tCwcrmRbJd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#saving the model\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Of-o4VOATJFi",
    "outputId": "5faf1c96-0c88-47fa-f64d-61bd55c5594f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the model\n",
    "model_new = WideDeep(deeptabular=tab_mlp)\n",
    "model_new.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n",
    "#Instantiate the trainer\n",
    "trainer_new = Trainer(model_new, objective=\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLDHzX_kK3_0",
    "outputId": "4969f66e-edd8-4e9a-fc7e-7abe38a37822",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:07<00:00, 160.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#predict on test\n",
    "X_tab_test = tab_preprocessor.transform(test_df)\n",
    "pred = trainer.predict(X_tab = X_tab_test, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAhAgvMC07g6",
    "outputId": "9721a741-7f48-4494-f5d5-ba8717ab1a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  96494.8889646102\n",
      "R-squared:  0.6746827057046232\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "y = test_df[\"resale_price\"]\n",
    "\n",
    "#calculating rmse\n",
    "rmse = np.sqrt(mean_squared_error(y, pred))\n",
    "print(\"RMSE: \", rmse)\n",
    "\n",
    "#calculating R-squared\n",
    "r_squared = r2_score(y, pred)\n",
    "print(\"R-squared: \", r_squared)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
